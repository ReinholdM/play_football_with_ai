# battle: https://github.com/wsjeon/maddpg-rllib
group: "Football"
name: "mappo/grfootball_marl"

worker_config:
  worker_num: 1

training:
  interface:
    type: "sync"
    population_size: -1
    worker_config:
      num_gpus: 1.0
  config:
    # control the frequency of remote parameter update
    update_interval: 1
    saving_interval: 100 # the sequence of dump model's weight
    batch_size: 200 # how many data sample from DatasetServer per time.
    optimizer: "Adam"
    actor_lr: 1.e-4
    critic_lr: 1.e-4
  
  ## initial policy to the populations
  # initial_policy:
  #   - policy_id: goal_fucker
  #     policy_dir: models/run_and_goal # the initial policy dir

  use_bot: False # this will control whether to use built-in-ai


rollout:
  type: "sync"
  stopper: "simple_rollout"
  stopper_config:
    max_step: 10
  metric_type: "grf"
  fragment_length: 3001 # 这个值现在是多少，就采多少步，1000的scenario完整的就是1001步
  num_episodes: 200
  episode_seg: 1 # 要去看看用的是DummyVecEnv还是其他
  terminate: "any"


evaluation:
  max_episode_length: -1
  num_episodes: 10

env_description:
  id: "BaseGFootBall"
  config:
    use_built_in_GK: True
    scenario_config:
      env_name: "5_vs_5"
      number_of_left_players_agent_controls: 4
      number_of_right_players_agent_controls: 4
      representation: "raw"
      stacked: False
      logdir: '/tmp/football/malib_psro'
      write_goal_dumps: False
      write_full_episode_dumps: False
      render: False
  #    other_config_options:
  #      action_set: v2

benchmark_env_description:
  id: "BenchmarkGFootball"
  config:
    use_built_in_GK: True
    scenario_config:
      env_name: "5_vs_5"
      number_of_left_players_agent_controls: 4
      number_of_right_players_agent_controls: 0
      representation: "raw"
      stacked: False
      logdir: '/tmp/football/malib_psro_benchmark'
      write_goal_dumps: False
      write_full_episode_dumps: False
      render: False
  #    other_config_options:
  #      action_set: v2

algorithms:
  MAPPO:
    name: "MAPPO"
    model_config:
      initialization:
        use_orthogonal: True
        gain: 1.
      actor:
        network: mlp
        layers:
          - units: 32
            activation: ReLU
          - units: 32
            activation: ReLU
          - units: 32
            activation: ReLU
        output:
          activation: False
      critic:
        network: mlp
        layers:
          - units: 32
            activation: ReLU
          - units: 32
            activation: ReLU
          - units: 32
            activation: ReLU
        output:
          activation: False

    # set hyper parameter
    custom_config:
      gamma: 0.99
      use_cuda: False  # enable cuda or not
      use_q_head: False
      ppo_epoch: 30
      num_mini_batch: 2  # the number of mini-batches
      # use_naive_rnn: True


global_evaluator:
  name: "psro"
  config:
    stop_metrics:
      max_iteration: 20

dataset_config:
  episode_capacity: 200
